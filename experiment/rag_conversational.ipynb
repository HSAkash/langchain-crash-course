{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a023aa5d604f433eac6249f81fc215df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_843bafab7cce4c519fa163a3b8d97372",
              "IPY_MODEL_c2832df9ada44fa08e47061c40655ba4",
              "IPY_MODEL_dacda76f64224b8a8fe9bd59524967a5"
            ],
            "layout": "IPY_MODEL_04299d12ffde4803983a4bce88df80ae"
          }
        },
        "843bafab7cce4c519fa163a3b8d97372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ea7409d456340b1a6766c38f950c3a1",
            "placeholder": "​",
            "style": "IPY_MODEL_2893b63b925047d9822918e7a4b8ae88",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c2832df9ada44fa08e47061c40655ba4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_268b7121a7ed4c26959311fea9189644",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3fba2aa9e8fd4a7ba2694972d2d7fa9a",
            "value": 2
          }
        },
        "dacda76f64224b8a8fe9bd59524967a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3694765f6944490ca12c7aa084e36b87",
            "placeholder": "​",
            "style": "IPY_MODEL_15bffb06f1a145d8aeb4f8052f4bb1ca",
            "value": " 2/2 [00:40&lt;00:00, 18.32s/it]"
          }
        },
        "04299d12ffde4803983a4bce88df80ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ea7409d456340b1a6766c38f950c3a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2893b63b925047d9822918e7a4b8ae88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "268b7121a7ed4c26959311fea9189644": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fba2aa9e8fd4a7ba2694972d2d7fa9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3694765f6944490ca12c7aa084e36b87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15bffb06f1a145d8aeb4f8052f4bb1ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Chat model"
      ],
      "metadata": {
        "id": "TBkDVVTK33Ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Insatall dependencies"
      ],
      "metadata": {
        "id": "4336t4XS3yA4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x4fVk4C0x5JQ"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "! pip install langchain huggingface_hub langchain_community chroma tiktoken sentence_transformers chromadb langchain_huggingface\n",
        "# !pip install flash-attn --no-build-isolation\n",
        "IPython.display.clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip -q \"/content/drive/MyDrive/Colab Notebooks/Chatbot/db.zip\""
      ],
      "metadata": {
        "id": "fx0V0cZbyB7B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import torch\n",
        "import os\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
      ],
      "metadata": {
        "id": "F721PFn4yWsb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding model"
      ],
      "metadata": {
        "id": "kVWq_gJj3-Hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzI5ov37yd-W",
        "outputId": "fef60f97-e7d9-40ca-9098-8e0d1eaf8250"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-59bedd9487e9>:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM model"
      ],
      "metadata": {
        "id": "h8WdylDU4BRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flan-t5"
      ],
      "metadata": {
        "id": "JeO6owRDCTvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"google/flan-t5-xl\"\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path, torch_dtype=torch.float16 if device == 0 else torch.float32)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Set up text generation pipeline with reduced tokens\n",
        "text_generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=128, device=device)\n",
        "\n",
        "# HuggingFacePipeline for chat\n",
        "llm = HuggingFacePipeline(pipeline=text_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a023aa5d604f433eac6249f81fc215df",
            "843bafab7cce4c519fa163a3b8d97372",
            "c2832df9ada44fa08e47061c40655ba4",
            "dacda76f64224b8a8fe9bd59524967a5",
            "04299d12ffde4803983a4bce88df80ae",
            "5ea7409d456340b1a6766c38f950c3a1",
            "2893b63b925047d9822918e7a4b8ae88",
            "268b7121a7ed4c26959311fea9189644",
            "3fba2aa9e8fd4a7ba2694972d2d7fa9a",
            "3694765f6944490ca12c7aa084e36b87",
            "15bffb06f1a145d8aeb4f8052f4bb1ca"
          ]
        },
        "id": "k5xKYd5fyfuf",
        "outputId": "ef6728f1-e681-4b9a-87ca-f0916f678f03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a023aa5d604f433eac6249f81fc215df"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load vector db"
      ],
      "metadata": {
        "id": "_tRvnr0r4D6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persistent_directory = os.path.join(\"db\", \"chroma_db_with_metadata\")"
      ],
      "metadata": {
        "id": "qT05zD5mz1Ru"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = Chroma(persist_directory=persistent_directory, embedding_function=embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT-ZK3Ovz8D6",
        "outputId": "0af1d6e0-ade7-41a8-dfb4-60c40e4607e4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-d0bd6a145aea>:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  db = Chroma(persist_directory=persistent_directory, embedding_function=embeddings)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a retriever for querying the vector store\n",
        "* `search_type` specifies the type of search (e.g., similarity)\n",
        "* `search_kwargs` contains additional arguments for the search (e.g., number of results to return)"
      ],
      "metadata": {
        "id": "q8R_CX0I4Kx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3},\n",
        ")"
      ],
      "metadata": {
        "id": "rbnsaeHlz-nb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contextualize question prompt\n",
        "* This system prompt helps the AI understand that it should reformulate the question\n",
        "* based on the chat history to make it a standalone question"
      ],
      "metadata": {
        "id": "UQ1GKxRY4Pco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, just \"\n",
        "    \"reformulate it if needed and otherwise return it as is.\"\n",
        ")\n",
        "\n",
        "# Create a prompt template for contextualizing questions\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Ez8g5Ou30GZL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a history-aware retriever\n",
        "* This uses the LLM to help reformulate the question based on chat history"
      ],
      "metadata": {
        "id": "BoPX2PYg4T9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")\n"
      ],
      "metadata": {
        "id": "QiDcTETx0Z-D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test history aware retriever"
      ],
      "metadata": {
        "id": "MQX7KI_b4fPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "query = input(\"You: \")\n",
        "response = history_aware_retriever.invoke({\"input\": query, \"chat_history\": chat_history})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh3n30Xb0c61",
        "outputId": "c2d3ba5c-c09a-44ee-fb0a-2eb525fc43d5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: I also just finishsed romeo and juliet. How did she die?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg9hYlaO0xSp",
        "outputId": "9f27e668-eb6b-43af-ba5d-4ecf7572f722"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'romeo_and_juliet.txt'}, page_content='FRIAR LAWRENCE.\\nI hear some noise. Lady, come from that nest\\nOf death, contagion, and unnatural sleep.\\nA greater power than we can contradict\\nHath thwarted our intents. Come, come away.\\nThy husband in thy bosom there lies dead;\\nAnd Paris too. Come, I’ll dispose of thee\\nAmong a sisterhood of holy nuns.\\nStay not to question, for the watch is coming.\\nCome, go, good Juliet. I dare no longer stay.\\n\\nJULIET.\\nGo, get thee hence, for I will not away.\\n\\n [_Exit Friar Lawrence._]\\n\\nWhat’s here? A cup clos’d in my true love’s hand?\\nPoison, I see, hath been his timeless end.\\nO churl. Drink all, and left no friendly drop\\nTo help me after? I will kiss thy lips.\\nHaply some poison yet doth hang on them,\\nTo make me die with a restorative.\\n\\n [_Kisses him._]\\n\\nThy lips are warm!\\n\\nFIRST WATCH.\\n[_Within._] Lead, boy. Which way?\\n\\nJULIET.\\nYea, noise? Then I’ll be brief. O happy dagger.\\n\\n [_Snatching Romeo’s dagger._]\\n\\nThis is thy sheath. [_stabs herself_] There rest, and let me die.'),\n",
              " Document(metadata={'source': 'romeo_and_juliet.txt'}, page_content='Enter Capulet.\\n\\nCAPULET.\\nFor shame, bring Juliet forth, her lord is come.\\n\\nNURSE.\\nShe’s dead, deceas’d, she’s dead; alack the day!\\n\\nLADY CAPULET.\\nAlack the day, she’s dead, she’s dead, she’s dead!\\n\\nCAPULET.\\nHa! Let me see her. Out alas! She’s cold,\\nHer blood is settled and her joints are stiff.\\nLife and these lips have long been separated.\\nDeath lies on her like an untimely frost\\nUpon the sweetest flower of all the field.\\n\\nNURSE.\\nO lamentable day!\\n\\nLADY CAPULET.\\nO woful time!\\n\\nCAPULET.\\nDeath, that hath ta’en her hence to make me wail,\\nTies up my tongue and will not let me speak.\\n\\n Enter Friar Lawrence and Paris with Musicians.\\n\\nFRIAR LAWRENCE.\\nCome, is the bride ready to go to church?\\n\\nCAPULET.\\nReady to go, but never to return.\\nO son, the night before thy wedding day\\nHath death lain with thy bride. There she lies,\\nFlower as she was, deflowered by him.\\nDeath is my son-in-law, death is my heir;\\nMy daughter he hath wedded. I will die\\nAnd leave him all; life, living, all is death’s.'),\n",
              " Document(metadata={'source': 'romeo_and_juliet.txt'}, page_content='SECOND MUSICIAN.\\nHang him, Jack. Come, we’ll in here, tarry for the mourners, and stay\\ndinner.\\n\\n [_Exeunt._]\\n\\n\\nACT V\\n\\nSCENE I. Mantua. A Street.\\n\\n\\n Enter Romeo.\\n\\nROMEO.\\nIf I may trust the flattering eye of sleep,\\nMy dreams presage some joyful news at hand.\\nMy bosom’s lord sits lightly in his throne;\\nAnd all this day an unaccustom’d spirit\\nLifts me above the ground with cheerful thoughts.\\nI dreamt my lady came and found me dead,—\\nStrange dream, that gives a dead man leave to think!—\\nAnd breath’d such life with kisses in my lips,\\nThat I reviv’d, and was an emperor.\\nAh me, how sweet is love itself possess’d,\\nWhen but love’s shadows are so rich in joy.\\n\\n Enter Balthasar.\\n\\nNews from Verona! How now, Balthasar?\\nDost thou not bring me letters from the Friar?\\nHow doth my lady? Is my father well?\\nHow fares my Juliet? That I ask again;\\nFor nothing can be ill if she be well.')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer question prompt\n",
        "* This system prompt helps the AI understand that it should provide concise answers\n",
        "* based on the retrieved context and indicates what to do if the answer is unknown"
      ],
      "metadata": {
        "id": "VYLqCSFp4mCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. Use \"\n",
        "    \"the following pieces of retrieved context to answer the \"\n",
        "    \"question. If you don't know the answer, just say that you \"\n",
        "    \"don't know. Use three sentences maximum and keep the answer \"\n",
        "    \"concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "cma--3ZZ017C"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a prompt template for answering questions"
      ],
      "metadata": {
        "id": "7eTezG_14ppY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "t9T7gNt01MZX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a chain to combine documents for question answering\n",
        "* `create_stuff_documents_chain` feeds all retrieved context into the LLM"
      ],
      "metadata": {
        "id": "i3omI5WN4tBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)"
      ],
      "metadata": {
        "id": "kB2vVb_21Nk9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a retrieval chain that combines the history-aware retriever and the question answering chain"
      ],
      "metadata": {
        "id": "_846mXWS4wFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
      ],
      "metadata": {
        "id": "llo5x8Zn1T_d"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to simulate a continual chat"
      ],
      "metadata": {
        "id": "XtB4siqo4z5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start chatting with the AI! Type 'exit' to end the conversation.\")\n",
        "chat_history = []  # Collect chat history here (a sequence of messages)\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    # Process the user's query through the retrieval chain\n",
        "    result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
        "    # Display the AI's response\n",
        "    print(f\"AI: {result['answer']}\")\n",
        "    # Update the chat history\n",
        "    chat_history.append(HumanMessage(content=query))\n",
        "    chat_history.append(SystemMessage(content=result[\"answer\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTfMzDTx1WKn",
        "outputId": "ebe5fee6-59d5-48d3-9cb5-edab433d8a8b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start chatting with the AI! Type 'exit' to end the conversation.\n",
            "You: I also just finishsed romeo and juliet. How did she die?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (993 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI: stabs herself\n",
            "You: who is brandon again?\n",
            "AI: not enough information\n",
            "You: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n",
        "\n",
        "# import transformers\n",
        "# from langchain.chains import RetrievalQA\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "# from langchain.document_loaders import TextLoader\n",
        "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "# from langchain.indexes import VectorstoreIndexCreator\n",
        "# from langchain.llms import HuggingFacePipeline\n",
        "# from langchain.prompts import PromptTemplate\n",
        "# from langchain.schema import (\n",
        "#     HumanMessage,\n",
        "#     SystemMessage,\n",
        "# )\n",
        "# from langchain.vectorstores import FAISS\n",
        "# from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "\n",
        "\n",
        "# def create_stuff_documents_chain(llm, qa_prompt):\n",
        "#     # Load the pre-trained model and tokenizer\n",
        "#     model_name = \"distilbert-base-uncased-distilled-squad\"\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#     model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "#     # Create the pipeline\n",
        "#     nlp = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
        "#     hf_llm = HuggingFacePipeline(pipeline=nlp)\n",
        "\n",
        "#     chain = RetrievalQA.from_chain_type(\n",
        "#         llm=hf_llm,\n",
        "#         chain_type=\"stuff\",  # Use \"stuff\" chain type to insert context into the prompt\n",
        "#         retriever=None,  # We will use history_aware_retriever in the next step\n",
        "#         chain_type_kwargs={\"prompt\": qa_prompt},\n",
        "#     )\n",
        "#     return chain\n",
        "\n",
        "\n",
        "# def create_retrieval_chain(retriever, question_answer_chain):\n",
        "#     chain = RetrievalQA.from_chain_type(\n",
        "#         llm=question_answer_chain.llm,\n",
        "#         chain_type=\"stuff\",\n",
        "#         retriever=retriever,\n",
        "#         chain_type_kwargs={\"prompt\": question_answer_chain.prompt},\n",
        "#     )\n",
        "#     return chain\n",
        "\n",
        "\n",
        "# # Assuming qa_prompt and history_aware_retriever are defined elsewhere\n",
        "# # ... (Your existing code for loading documents, creating retriever, qa_prompt, etc.)\n",
        "\n",
        "# # Initialize the chat model and create the chains\n",
        "# llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "# question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "# rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
        "\n",
        "# print(\"Start chatting with the AI! Type 'exit' to end the conversation.\")\n",
        "# chat_history = []  # Collect chat history here (a sequence of messages)\n",
        "# while True:\n",
        "#     query = input(\"You: \")\n",
        "#     if query.lower() == \"exit\":\n",
        "#         break\n",
        "#     # Process the user's query through the retrieval chain\n",
        "\n",
        "#     # This part needs modification to fit the expected input format\n",
        "#     # Assuming 'input' is the query and chat_history provides context:\n",
        "#     result = rag_chain({\"query\": query, \"chat_history\": chat_history})\n",
        "#     # Display the AI's response\n",
        "#     print(f\"AI: {result['result']}\")  # Access the result with 'result' key\n",
        "#     # Update the chat history\n",
        "#     chat_history.append(HumanMessage(content=query))\n"
      ],
      "metadata": {
        "id": "UUvjX59UEOLq"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}